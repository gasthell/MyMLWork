{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":44631,"databundleVersionId":4924039,"sourceType":"competition"}],"dockerImageVersionId":31040,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# VIT-B-16 or VGG16 - pytorch","metadata":{}},{"cell_type":"markdown","source":"## *Need some changes only in config*","metadata":{}},{"cell_type":"code","source":"# Standard library\nimport copy\nimport glob\nimport multiprocessing\nimport os\nimport time\nimport zipfile\n\n# Pytorch\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import models, transforms\n\n# Related third party\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nfrom skimage import io, transform\nfrom sklearn.model_selection import train_test_split\nfrom tqdm.notebook import tqdm\n\ninput_size = 224\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\n\nnum_classes = 10\n\ntable_for_convert = {0: 'airplane', 1: 'automobile', 2: 'bird', 3: 'cat', 4: 'deer', 5: 'dog', 6: 'frog', 7: 'horse', 8: 'ship', 9: 'truck'}\n#Need only when multiclass, this snippet will help you to extract this var:\n'''\ntemp = CIFAR10Dataset(train_list, transform=data_transforms['train'])\ntemp.idx_to_class\n'''\n\nbatch_size = 32\nnum_epochs = 5\n\nfinetune = True\n\nnum_workers = multiprocessing.cpu_count()\n\ndevice = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n\ndef train_model(model, dataloaders, criterion, optimizer, num_epochs=25):\n    since = time.time()\n\n    history = {'accuracy': [],\n               'val_accuracy': [],\n               'loss': [],\n               'val_loss': []}\n\n    best_model_wts = copy.deepcopy(model.state_dict())\n    best_acc = 0.0\n\n    for epoch in range(num_epochs):\n        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n        print('-' * 10)\n\n        # Each epoch has a training and validation phase\n        for phase in ['train', 'val']:\n            if phase == 'train':\n                model.train()  # Set model to training mode\n            else:\n                model.eval()   # Set model to evaluate mode\n\n            running_loss = 0.0\n            running_corrects = 0\n\n            # Iterate over data.\n            for inputs, labels in tqdm(dataloaders[phase]):\n                inputs = inputs.to(device)\n                labels = labels.to(device)\n\n                # zero the parameter gradients\n                optimizer.zero_grad()\n\n                # forward\n                # track history if only in train\n                with torch.set_grad_enabled(phase == 'train'):\n                    outputs = model(inputs)\n                    loss = criterion(outputs, labels)\n\n                    _, preds = torch.max(outputs, 1)\n\n                    # backward + optimize only if in training phase\n                    if phase == 'train':\n                        loss.backward()\n                        optimizer.step()\n\n                # statistics\n                running_loss += loss.item() * inputs.size(0)\n                running_corrects += torch.sum(preds == labels.data)\n\n            epoch_loss = running_loss / len(dataloaders[phase].dataset)\n            epoch_acc = running_corrects.double() / len(dataloaders[phase].dataset)\n\n            print('{} Loss: {:.4f} Acc: {:.4f}'.format(phase, epoch_loss, epoch_acc))\n\n            # deep copy the model\n            if phase == 'val' and epoch_acc > best_acc:\n                best_acc = epoch_acc\n                best_model_wts = copy.deepcopy(model.state_dict())\n\n            if phase == 'train':\n                history['accuracy'].append(epoch_acc.item())\n                history['loss'].append(epoch_loss)\n            else:\n                history['val_accuracy'].append(epoch_acc.item())\n                history['val_loss'].append(epoch_loss) \n\n        print()\n\n    time_elapsed = time.time() - since\n    print('Training complete in {:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n    print('Best val Acc: {:4f}'.format(best_acc))\n\n    # load best model weights\n    model.load_state_dict(best_model_wts)\n    return model, history","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## *Data depended*","metadata":{}},{"cell_type":"code","source":"class CIFAR10Dataset(Dataset):\n  \n    def __init__(self, file_list, transform=None):\n        self.file_list = file_list\n        self.transform = transform\n        \n        # Load the labels file\n        self.labels_df = pd.read_csv(\"/kaggle/working/label.csv\") # or throw labels directly\n        \n        # Create a mapping from class names ('cat', 'dog') to integer indices (0, 1)\n        # This is essential for training with loss functions like CrossEntropyLoss\n        self.classes = sorted(self.labels_df['label'].unique())\n        self.class_to_idx = {cls_name: i for i, cls_name in enumerate(self.classes)}\n        self.idx_to_class = {i: cls_name for cls_name, i in self.class_to_idx.items()}\n\n        # Set the 'id' column as the index for fast lookup later\n        self.labels_df = self.labels_df.set_index('id')\n    \n    def __len__(self):\n        return len(self.file_list)\n  \n    def __getitem__(self, idx):\n        if torch.is_tensor(idx):\n            idx = idx.tolist()\n       \n        img_name = self.file_list[idx]\n        image = Image.open(img_name)\n        if self.transform:\n            image = self.transform(image)\n    \n        # 1. Extract the image ID from the filename.\n        img_id_str = os.path.basename(img_name).split('.')[0]\n        img_id = int(img_id_str)\n        \n        # 2. Look up the string label (e.g., 'cat') from the DataFrame.\n        label_name = self.labels_df.loc[img_id, 'label']\n        \n        # 3. Convert the string label to its corresponding integer index.\n        label = self.class_to_idx[label_name]\n    \n        return image, label\n\nall_train_files = glob.glob(os.path.join(\"/kaggle/working/train\", '*.jpg'))\ntrain_list, val_list = train_test_split(all_train_files, random_state=42)\n\nprint(len(train_list))\nprint(len(val_list))\n\ndata_transforms = {\n    'train': transforms.Compose([\n        #transforms.Lambda(lambda img: img.convert(\"RGB\")),\n        transforms.RandomResizedCrop(input_size, scale=(0.5, 1.0)),\n        transforms.RandomHorizontalFlip(),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ]),\n    'val': transforms.Compose([\n        #transforms.Lambda(lambda img: img.convert(\"RGB\")),\n        transforms.Resize(input_size),\n        transforms.CenterCrop(input_size),\n        transforms.ToTensor(),\n        transforms.Normalize(mean, std)\n    ])\n}\n\nimage_datasets = {\n    'train': CIFAR10Dataset(train_list,\n                             transform=data_transforms['train']),\n    'val': CIFAR10Dataset(val_list,\n                           transform=data_transforms['val'])\n}\n\ndataloaders_dict = {x: DataLoader(image_datasets[x],\n                                  batch_size=batch_size,\n                                  shuffle=True,\n                                  num_workers=num_workers) for x in ['train', 'val']}","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"## *Stay same*","metadata":{}},{"cell_type":"code","source":"# weights=None if without pretrained","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"model_ft = models.vgg16(pretrained=True)\nmodel_ft.classifier[6] = nn.Linear(4096, num_classes)\n\n'''\nmodel_ft = models.vit_b_16(pretrained=True)\nmodel_ft.heads = nn.Sequential(\n    nn.Linear(768, 4096),\n    nn.ReLU(),\n    nn.Linear(4096, num_classes)\n) # or nn.Linear(768, num_classes)\n'''\n\nmodel_ft = model_ft.to(device)\n\nparams_to_update = model_ft.parameters()\nprint(\"Params to learn:\")\nif finetune:\n    params_to_update = []\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            params_to_update.append(param)\n            print(\"\\t\",name)\nelse:\n    for name,param in model_ft.named_parameters():\n        if param.requires_grad == True:\n            print(\"\\t\",name)\n\n# Observe that all parameters are being optimized\noptimizer_ft = optim.SGD(params_to_update, lr=0.001, momentum=0.9)\n\n# Setup the loss fxn\ncriterion = nn.CrossEntropyLoss()\n\n# Train and evaluate\nmodel_ft, hist = train_model(model_ft, dataloaders_dict, criterion, optimizer_ft, num_epochs=num_epochs)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_list = glob.glob(os.path.join(\"/kaggle/working/test\", '*.png'))\ntest_data_transform = data_transforms['val']\n\nlabels = []\nsubmission_ids = []\n\nwith torch.no_grad():\n    # Loop through all the test images\n    for test_path in tqdm(test_list):\n        # 1. Get the image ID from the filename\n        # e.g., from '/path/to/123.png', get 123\n        img_id = int(os.path.basename(test_path).split('.')[0])\n        submission_ids.append(img_id)\n        \n        # 2. Load and transform the image\n        img = Image.open(test_path).convert(\"RGB\") # Ensure it's RGB\n        img = test_data_transform(img)\n        \n        # Add a batch dimension (C, H, W) -> (1, C, H, W) and send to device\n        img = img.unsqueeze(0)\n        img = img.to(device)\n\n        # 3. Get model predictions (logits)\n        outputs = model_ft(img)\n\n        # 4. Get the predicted class index by finding the max logit\n        _, pred_idx_tensor = torch.max(outputs, 1)\n        pred_idx = pred_idx_tensor.item() # .item() gets the integer from the tensor\n\n        labels.append(pred_idx)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"test_list = glob.glob(os.path.join(\"/kaggle/working/test\", '*.jpg'))\ntest_data_transform = data_transforms['val']\n\nids = []\nlabels = []\n\nwith torch.no_grad():\n    for test_path in tqdm(test_list):\n        img = Image.open(test_path)\n        img = test_data_transform(img)\n        img = img.unsqueeze(0)\n        img = img.to(device)\n\n        model_ft.eval()\n        outputs = model_ft(img)\n        preds = F.softmax(outputs, dim=1)[:, 1].tolist()\n\n        test_id = extract_class_from(test_path)\n        ids.append(int(test_id))\n        labels.append(preds[0])","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"class_labels = list(map(table_for_convert.get, labels))","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Machine Learning","metadata":{}},{"cell_type":"code","source":"# Core\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\n%matplotlib inline\nimport seaborn as sns\nsns.set(style='darkgrid', font_scale=1.4)\nimport itertools\nimport warnings\nwarnings.filterwarnings('ignore')\nimport plotly.express as px\nimport time\n\n# Sklearn\nfrom sklearn.model_selection import train_test_split, GridSearchCV, RandomizedSearchCV, StratifiedKFold, PredefinedSplit\nfrom sklearn.metrics import accuracy_score, confusion_matrix, recall_score, precision_score, f1_score\nfrom sklearn.metrics import roc_auc_score, roc_curve\nfrom sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder\nfrom sklearn.feature_selection import mutual_info_classif\nfrom sklearn.decomposition import PCA\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.impute import SimpleImputer\nimport eli5\nfrom eli5.sklearn import PermutationImportance\nfrom sklearn.utils import resample\n\n# Models\nfrom sklearn.linear_model import LinearRegression, LogisticRegression\nfrom sklearn.neighbors import KNeighborsClassifier\nfrom sklearn.svm import SVC\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.ensemble import RandomForestClassifier, ExtraTreesClassifier\nfrom xgboost import XGBClassifier\nfrom lightgbm import LGBMClassifier\nfrom catboost import CatBoostClassifier\nfrom sklearn.naive_bayes import GaussianNB","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:22:55.109139Z","iopub.execute_input":"2025-06-25T02:22:55.109508Z","iopub.status.idle":"2025-06-25T02:22:55.125960Z","shell.execute_reply.started":"2025-06-25T02:22:55.109478Z","shell.execute_reply":"2025-06-25T02:22:55.124865Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load data\ntrain = pd.read_csv(\"/kaggle/input/playground-series-s3e3/train.csv\", index_col = \"id\")\ntest = pd.read_csv(\"/kaggle/input/playground-series-s3e3/test.csv\", index_col = \"id\")\nsub = pd.read_csv(\"/kaggle/input/playground-series-s3e3/sample_submission.csv\", index_col = \"id\")\ntrain.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:22:55.928050Z","iopub.execute_input":"2025-06-25T02:22:55.928386Z","iopub.status.idle":"2025-06-25T02:22:55.965200Z","shell.execute_reply.started":"2025-06-25T02:22:55.928362Z","shell.execute_reply":"2025-06-25T02:22:55.964148Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"train_full = train\ntrain_full.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:21.690492Z","iopub.execute_input":"2025-06-25T02:23:21.691083Z","iopub.status.idle":"2025-06-25T02:23:21.697151Z","shell.execute_reply.started":"2025-06-25T02:23:21.691057Z","shell.execute_reply":"2025-06-25T02:23:21.696244Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Preview data\ntrain_full.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:23.569411Z","iopub.execute_input":"2025-06-25T02:23:23.570364Z","iopub.status.idle":"2025-06-25T02:23:23.605381Z","shell.execute_reply.started":"2025-06-25T02:23:23.570336Z","shell.execute_reply":"2025-06-25T02:23:23.604432Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Drop constant columns\ntrain_full.drop([\"EmployeeCount\", \"Over18\", \"StandardHours\"], axis=1, inplace=True)\ntest.drop([\"EmployeeCount\", \"Over18\", \"StandardHours\"], axis=1, inplace=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:25.557821Z","iopub.execute_input":"2025-06-25T02:23:25.558162Z","iopub.status.idle":"2025-06-25T02:23:25.566014Z","shell.execute_reply.started":"2025-06-25T02:23:25.558138Z","shell.execute_reply":"2025-06-25T02:23:25.564936Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Split features and labels\ny = train_full[\"Attrition\"]\nX = train_full.drop(\"Attrition\", axis=1)\nX_test = test","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:26.494127Z","iopub.execute_input":"2025-06-25T02:23:26.494583Z","iopub.status.idle":"2025-06-25T02:23:26.500909Z","shell.execute_reply.started":"2025-06-25T02:23:26.494537Z","shell.execute_reply":"2025-06-25T02:23:26.499813Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Categorical columns\ncat_cols = [col for (col, d) in zip(X.columns,X.dtypes) if d == \"object\"]\ncat_cols","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:29.825374Z","iopub.execute_input":"2025-06-25T02:23:29.825726Z","iopub.status.idle":"2025-06-25T02:23:29.832895Z","shell.execute_reply.started":"2025-06-25T02:23:29.825701Z","shell.execute_reply":"2025-06-25T02:23:29.831988Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Encode cat columns\nX = pd.get_dummies(X)\nX_test = pd.get_dummies(X_test)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:30.184885Z","iopub.execute_input":"2025-06-25T02:23:30.185267Z","iopub.status.idle":"2025-06-25T02:23:30.208881Z","shell.execute_reply.started":"2025-06-25T02:23:30.185243Z","shell.execute_reply":"2025-06-25T02:23:30.207872Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Scale data\nscaler = StandardScaler()\nX = pd.DataFrame(scaler.fit_transform(X), columns = X.columns)\nX_test = pd.DataFrame(scaler.transform(X_test), columns = X_test.columns)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:31.890110Z","iopub.execute_input":"2025-06-25T02:23:31.890399Z","iopub.status.idle":"2025-06-25T02:23:31.924238Z","shell.execute_reply.started":"2025-06-25T02:23:31.890381Z","shell.execute_reply":"2025-06-25T02:23:31.923295Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# PCA\npca = PCA()\nX_pca = pca.fit_transform(X)\npca_df = pd.DataFrame(data = X_pca)\n\n# Scatterplot\nplt.figure(figsize=(8,6))\nplt.scatter(pca_df.iloc[:,0], pca_df.iloc[:,1], c=y, cmap=\"brg\", s=40)\nplt.title('PCA plot in 2D')\nplt.xlabel('principal component 1')\nplt.ylabel('principal component 2')\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:33.149945Z","iopub.execute_input":"2025-06-25T02:23:33.150291Z","iopub.status.idle":"2025-06-25T02:23:33.621932Z","shell.execute_reply.started":"2025-06-25T02:23:33.150269Z","shell.execute_reply":"2025-06-25T02:23:33.620932Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Plot\nplt.figure(figsize=(14,5))\nxi = np.arange(1,1+X.shape[1], step=1)\nyi = np.cumsum(pca.explained_variance_ratio_)\nplt.plot(xi, yi, marker='o', linestyle='--', color='b')\n\n# Aesthetics\nplt.ylim(0.0,1.1)\nplt.xlabel('Number of Components')\nplt.xticks(np.arange(1, 1+X.shape[1], step=2))\nplt.ylabel('Cumulative variance (%)')\nplt.title('Explained variance by each component')\nplt.axhline(y=1, color='r', linestyle='-')\nplt.gca().xaxis.grid(False)\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:36.191070Z","iopub.execute_input":"2025-06-25T02:23:36.191421Z","iopub.status.idle":"2025-06-25T02:23:36.603315Z","shell.execute_reply.started":"2025-06-25T02:23:36.191396Z","shell.execute_reply":"2025-06-25T02:23:36.602268Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"classifiers = {\n    \"LogisticRegression\" : LogisticRegression(random_state=0),\n    \"KNN\" : KNeighborsClassifier(),\n    \"SVC\" : SVC(random_state=0, probability=True),\n    \"RandomForest\" : RandomForestClassifier(random_state=0),\n    \"ExtraTrees\" : ExtraTreesClassifier(random_state=0),\n    \"XGBoost\" : XGBClassifier(random_state=0, use_label_encoder=False, eval_metric='logloss'),\n    \"LGBM\" : LGBMClassifier(random_state=0),\n    \"CatBoost\" : CatBoostClassifier(random_state=0, verbose=False),\n    \"NaiveBayes\": GaussianNB()\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:39.792320Z","iopub.execute_input":"2025-06-25T02:23:39.792683Z","iopub.status.idle":"2025-06-25T02:23:39.801811Z","shell.execute_reply.started":"2025-06-25T02:23:39.792658Z","shell.execute_reply":"2025-06-25T02:23:39.800344Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Grids for grid search\nLR_grid = {'penalty': ['l1','l2'],\n           \"solver\": [\"liblinear\"],\n           'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5]}\n\nKNN_grid = {'n_neighbors': [3, 5, 7, 9],\n            'p': [1, 2]}\n\nSVC_grid = {'C': [0.25, 0.5, 0.75, 1, 1.25, 1.5],\n            'kernel': ['linear', 'rbf'],\n            'gamma': ['scale', 'auto']}\n\nRF_grid = {'n_estimators': [50, 100, 150, 200, 250, 300],\n        'max_depth': [4, 6, 8, 10, 12]}\n\nboosted_grid = {'n_estimators': [50, 100, 150, 200],\n        'max_depth': [4, 8, 12],\n        'learning_rate': [0.05, 0.1, 0.15]}\n\nNB_grid={'var_smoothing': [1e-10, 1e-9, 1e-8, 1e-7]}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:41.481883Z","iopub.execute_input":"2025-06-25T02:23:41.482232Z","iopub.status.idle":"2025-06-25T02:23:41.489321Z","shell.execute_reply.started":"2025-06-25T02:23:41.482205Z","shell.execute_reply":"2025-06-25T02:23:41.488217Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Dictionary of all grids\ngrid = {\n    \"LogisticRegression\" : LR_grid,\n    \"KNN\" : KNN_grid,\n    \"SVC\" : SVC_grid,\n    \"RandomForest\" : RF_grid,\n    \"ExtraTrees\" : RF_grid,\n    \"XGBoost\" : boosted_grid,\n    \"LGBM\" : boosted_grid,\n    \"CatBoost\" : boosted_grid,\n    \"NaiveBayes\": NB_grid\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:43.018363Z","iopub.execute_input":"2025-06-25T02:23:43.018731Z","iopub.status.idle":"2025-06-25T02:23:43.024011Z","shell.execute_reply.started":"2025-06-25T02:23:43.018705Z","shell.execute_reply":"2025-06-25T02:23:43.022972Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"n_folds = 5\n\n# Train models\nfor key, classifier in classifiers.items():\n    # Initialise outputs\n    test_preds = np.zeros(len(X_test))\n    oof_full = y.copy()\n    \n    # Start timer\n    start = time.time()\n    \n    # k-fold cross validation\n    cv = StratifiedKFold(n_splits=n_folds, shuffle=True, random_state=0)\n    \n    score=0\n    for fold, (train_idx, val_idx) in enumerate(cv.split(X, y)):\n        # Get training and validation sets\n        X_train, X_valid = X.iloc[train_idx], X.iloc[val_idx]\n        y_train, y_valid = y.iloc[train_idx], y.iloc[val_idx]\n        \n        # Start timer\n        start = time.time()\n        \n        # Specify predefined validation set (-1 = train, 0 = valid)\n        #split_idx = np.zeros(len(y))\n        #split_idx[train_idx] = -1\n        #ps = PredefinedSplit(split_idx)\n        \n        # Tune hyperparameters\n        clf = RandomizedSearchCV(estimator=classifier, param_distributions=grid[key], n_iter=20, scoring='roc_auc', n_jobs=-1, cv=5)\n        \n        # Train using PredefinedSplit\n        clf.fit(X_train, y_train)\n        \n        # Out-of-fold predictions\n        oof_preds = clf.predict_proba(X_valid)[:,1]\n        score += roc_auc_score(y_valid, oof_preds)/n_folds\n        oof_full[val_idx] = oof_preds\n        \n        # Test set predictions\n        test_preds += clf.predict_proba(X_test)[:,1]/n_folds\n    \n    # Stop timer\n    stop = time.time()\n    \n    # Print score and time\n    print('Model:', key)\n    print('Average validation AUC:', np.round(100*score,2))\n    print('Training time (mins):', np.round((stop - start)/60,2))\n    print('')\n    \n    # Plot ROC curve\n    #plot_roc_curve(clf, X, y)\n    #plt.legend([key])\n    #plt.xlabel(\"False Positive Rate\")\n    #plt.ylabel(\"True Positive Rate\")\n    #plt.show()\n    \n    # Save oof and test set preds\n    oof_full.to_csv(f\"{key}_oof_preds.csv\", index=False)\n    ss = sub.copy()\n    ss[\"Attrition\"] = test_preds\n    ss.to_csv(f\"{key}_test_preds.csv\", index=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:23:44.098286Z","iopub.execute_input":"2025-06-25T02:23:44.098672Z","iopub.status.idle":"2025-06-25T02:52:36.470504Z","shell.execute_reply.started":"2025-06-25T02:23:44.098647Z","shell.execute_reply":"2025-06-25T02:52:36.469571Z"},"scrolled":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Join oof preds\noof_df = pd.DataFrame(index=np.arange(len(y)))\nfor i in classifiers.keys():\n    df = pd.read_csv(f\"/kaggle/working/{i}_oof_preds.csv\")\n    df.rename(columns={\"Attrition\": i}, inplace=True)\n    oof_df = pd.concat([oof_df,df], axis=1)\n    \n# Join test preds\ntest_preds = pd.DataFrame(index=np.arange(len(X_test)))\nfor i in classifiers.keys():\n    df = pd.read_csv(f\"/kaggle/working/{i}_test_preds.csv\")\n    df.rename(columns={\"Attrition\": i}, inplace=True)\n    test_preds = pd.concat([test_preds,df], axis=1)\n    \noof_df.head(3)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:52:36.472315Z","iopub.execute_input":"2025-06-25T02:52:36.472667Z","iopub.status.idle":"2025-06-25T02:52:36.531662Z","shell.execute_reply.started":"2025-06-25T02:52:36.472632Z","shell.execute_reply":"2025-06-25T02:52:36.530756Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Evaluate oof preds\nscores = {}\nfor col in oof_df.columns:\n    scores[col] = roc_auc_score(y, oof_df[col])\n\n# Sort scores\nscores = {k: v for k, v in sorted(scores.items(), key=lambda item: item[1], reverse=True)}\n\n# Sort oof_df and test_preds\noof_df = oof_df[list(scores.keys())]\ntest_preds = test_preds[list(scores.keys())]\n\nscores","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:52:36.532448Z","iopub.execute_input":"2025-06-25T02:52:36.532719Z","iopub.status.idle":"2025-06-25T02:52:36.563190Z","shell.execute_reply.started":"2025-06-25T02:52:36.532700Z","shell.execute_reply":"2025-06-25T02:52:36.562215Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Initialise\nSTOP = False\ncurrent_best_ensemble = oof_df.iloc[:,0]\ncurrent_best_test_preds = test_preds.iloc[:,0]\nMODELS = oof_df.iloc[:,1:]\nweight_range = np.arange(0.01,0.51,0.01)   # or with negative weights: np.arange(-0.5,0.51,0.01)\nhistory = [roc_auc_score(y, current_best_ensemble)]\ni=0\n\n# Hill climbing\nwhile not STOP:\n    i+=1\n    potential_new_best_cv_score = roc_auc_score(y, current_best_ensemble)\n    k_best, wgt_best = None, None\n    for k in MODELS:\n        for wgt in weight_range:\n            potential_ensemble = (1-wgt) * current_best_ensemble + wgt * MODELS[k]\n            cv_score = roc_auc_score(y, potential_ensemble)\n            if cv_score > potential_new_best_cv_score:\n                potential_new_best_cv_score = cv_score\n                k_best, wgt_best = k, wgt\n            \n    if k_best is not None:\n        current_best_ensemble = (1-wgt_best) * current_best_ensemble + wgt_best * MODELS[k_best]\n        current_best_test_preds = (1-wgt_best) * current_best_test_preds + wgt_best * test_preds[k_best]\n        MODELS.drop(k_best, axis=1, inplace=True)\n        if MODELS.shape[1]==0:\n            STOP = True\n        print(f'Iteration: {i}, Model added: {k_best}, Best weight: {wgt_best:.2f}, Best AUC: {potential_new_best_cv_score:.5f}')\n        history.append(potential_new_best_cv_score)\n    else:\n        STOP = True","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:52:47.949564Z","iopub.execute_input":"2025-06-25T02:52:47.950373Z","iopub.status.idle":"2025-06-25T02:52:51.077413Z","shell.execute_reply.started":"2025-06-25T02:52:47.950336Z","shell.execute_reply":"2025-06-25T02:52:51.076650Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nplt.plot(np.arange(len(history))+1, history, marker=\"x\")\nplt.title(\"CV AUC vs. Number of Models with Hill Climbing\")\nplt.xlabel(\"Number of models\")\nplt.ylabel(\"AUC\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:53:08.803587Z","iopub.execute_input":"2025-06-25T02:53:08.803935Z","iopub.status.idle":"2025-06-25T02:53:09.108979Z","shell.execute_reply.started":"2025-06-25T02:53:08.803912Z","shell.execute_reply":"2025-06-25T02:53:09.107492Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"plt.figure(figsize=(10,4))\nsns.histplot(current_best_test_preds)\nplt.title(\"Distribution of final predictions\")\nplt.show()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:53:14.197413Z","iopub.execute_input":"2025-06-25T02:53:14.198502Z","iopub.status.idle":"2025-06-25T02:53:14.480802Z","shell.execute_reply.started":"2025-06-25T02:53:14.198469Z","shell.execute_reply":"2025-06-25T02:53:14.479948Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Submit predictions\nsubmission = sub.copy()\nsubmission[\"Attrition\"] = current_best_test_preds.values\nsubmission.to_csv(\"submission.csv\", index=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-06-25T02:53:18.254533Z","iopub.execute_input":"2025-06-25T02:53:18.254912Z","iopub.status.idle":"2025-06-25T02:53:18.265853Z","shell.execute_reply.started":"2025-06-25T02:53:18.254885Z","shell.execute_reply":"2025-06-25T02:53:18.265001Z"}},"outputs":[],"execution_count":null}]}