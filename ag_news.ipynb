{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "1225b443",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tqdm.auto import tqdm\n",
    "import time\n",
    "import os\n",
    "import urllib.request\n",
    "import tarfile\n",
    "import csv\n",
    "from collections import Counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "7789ebe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from models.ctm_nlp import CTM_NLP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ff797a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {DEVICE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "eeeb9c08",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Параметры данных\n",
    "BATCH_SIZE = 64\n",
    "VOCAB_SIZE_LIMIT = 10000\n",
    "MAX_SEQ_LEN = 512\n",
    "\n",
    "# Параметры обучения\n",
    "EPOCHS = 3\n",
    "LEARNING_RATE = 0.001\n",
    "\n",
    "# Параметры моделей (остаются без изменений)\n",
    "CTM_D_MODEL = 256\n",
    "CTM_D_INPUT = 128\n",
    "CTM_ITERATIONS = 10\n",
    "CTM_HEADS = 4\n",
    "CTM_SYNCH_OUT = 128\n",
    "CTM_SYNCH_ACTION = 64\n",
    "CTM_SYNAPSE_DEPTH = 2\n",
    "CTM_MEMORY_LENGTH = 10\n",
    "CTM_MEMORY_HIDDEN = 32\n",
    "LSTM_HIDDEN_DIM = 128\n",
    "LSTM_NUM_LAYERS = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "03e04e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 2. Замена torchtext: Загрузка данных и создание словаря ---\n",
    "\n",
    "def download_and_extract_ag_news(root='./data'):\n",
    "    \"\"\"Скачивает и извлекает датасет AG_NEWS, если его нет.\"\"\"\n",
    "    url = \"https://s3.amazonaws.com/fast-ai-nlp/ag_news_csv.tgz\"\n",
    "    data_path = os.path.join(root, 'ag_news_csv')\n",
    "    \n",
    "    if os.path.exists(data_path):\n",
    "        print(\"Dataset already downloaded and extracted.\")\n",
    "    else:\n",
    "        print(\"Downloading AG_NEWS dataset...\")\n",
    "        os.makedirs(root, exist_ok=True)\n",
    "        tgz_path = os.path.join(root, 'ag_news_csv.tgz')\n",
    "        urllib.request.urlretrieve(url, tgz_path)\n",
    "        print(\"Extracting...\")\n",
    "        with tarfile.open(tgz_path, 'r:gz') as tar:\n",
    "            tar.extractall(path=root)\n",
    "        os.remove(tgz_path)\n",
    "        print(\"Done.\")\n",
    "        \n",
    "    train_data, test_data = [], []\n",
    "    with open(os.path.join(data_path, 'train.csv'), 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            # Класс, Заголовок, Описание\n",
    "            train_data.append((int(row[0]), row[1] + \" \" + row[2]))\n",
    "            \n",
    "    with open(os.path.join(data_path, 'test.csv'), 'r', encoding='utf-8') as f:\n",
    "        reader = csv.reader(f)\n",
    "        for row in reader:\n",
    "            test_data.append((int(row[0]), row[1] + \" \" + row[2]))\n",
    "            \n",
    "    return train_data, test_data\n",
    "\n",
    "def simple_tokenizer(text):\n",
    "    \"\"\"Простой токенизатор, который разделяет текст по пробелам.\"\"\"\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "def build_vocab(data, tokenizer, max_size):\n",
    "    \"\"\"Создает словарь (word -> index) на основе данных.\"\"\"\n",
    "    counter = Counter()\n",
    "    for _, text in data:\n",
    "        counter.update(tokenizer(text))\n",
    "    \n",
    "    # Создаем словарь с наиболее частыми словами\n",
    "    most_common_words = [word for word, _ in counter.most_common(max_size - 2)] # -2 для <pad> и <unk>\n",
    "    \n",
    "    # Добавляем специальные токены\n",
    "    word_to_idx = {'<pad>': 0, '<unk>': 1}\n",
    "    for i, word in enumerate(most_common_words):\n",
    "        word_to_idx[word] = i + 2\n",
    "        \n",
    "    return word_to_idx\n",
    "\n",
    "class NewsDataset(Dataset):\n",
    "    \"\"\"Простой класс датасета для PyTorch.\"\"\"\n",
    "    def __init__(self, data):\n",
    "        self.data = data\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "323ead55",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Модель Baseline: LSTM Classifier (без изменений) ---\n",
    "class LSTMClassifier(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim, hidden_dim, num_class, num_layers, pad_idx):\n",
    "        super(LSTMClassifier, self).__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=pad_idx)\n",
    "        self.lstm = nn.LSTM(embed_dim, hidden_dim, num_layers=num_layers, \n",
    "                            batch_first=True, bidirectional=True, dropout=0.3)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, num_class)\n",
    "\n",
    "    def forward(self, text):\n",
    "        embedded = self.embedding(text)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
    "        return self.fc(hidden)\n",
    "\n",
    "\n",
    "# --- 4. Функции обучения и оценки (без изменений) ---\n",
    "def train_epoch(model, dataloader, optimizer, criterion, model_type='lstm'):\n",
    "    model.train()\n",
    "    total_acc, total_loss, total_count = 0, 0, 0\n",
    "    progress_bar = tqdm(dataloader, desc=f'Training {model_type}')\n",
    "    for idx, (label, text) in enumerate(progress_bar):\n",
    "        label, text = label.to(DEVICE), text.to(DEVICE)\n",
    "        optimizer.zero_grad()\n",
    "        if model_type == 'ctm':\n",
    "            predictions, _, _ = model(text)\n",
    "            logits = predictions[:, :, -1]\n",
    "        else:\n",
    "            logits = model(text)\n",
    "        loss = criterion(logits, label)\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), 0.5)\n",
    "        optimizer.step()\n",
    "        total_acc += (logits.argmax(1) == label).sum().item()\n",
    "        total_loss += loss.item()\n",
    "        total_count += label.size(0)\n",
    "        progress_bar.set_postfix({'loss': total_loss / total_count, 'acc': total_acc / total_count})\n",
    "    return total_acc / total_count, total_loss / total_count\n",
    "\n",
    "def evaluate(model, dataloader, criterion, model_type='lstm'):\n",
    "    model.eval()\n",
    "    total_acc, total_loss, total_count = 0, 0, 0\n",
    "    with torch.no_grad():\n",
    "        for idx, (label, text) in enumerate(dataloader):\n",
    "            label, text = label.to(DEVICE), text.to(DEVICE)\n",
    "            if model_type == 'ctm':\n",
    "                predictions, _, _ = model(text)\n",
    "                logits = predictions[:, :, -1]\n",
    "            else:\n",
    "                logits = model(text)\n",
    "            loss = criterion(logits, label)\n",
    "            total_acc += (logits.argmax(1) == label).sum().item()\n",
    "            total_loss += loss.item()\n",
    "            total_count += label.size(0)\n",
    "    return total_acc / total_count, total_loss / total_count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "897d7577",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded and extracted.\n",
      "Vocabulary size: 10000\n"
     ]
    }
   ],
   "source": [
    "train_data, test_data = download_and_extract_ag_news()\n",
    "word_to_idx = build_vocab(train_data, simple_tokenizer, VOCAB_SIZE_LIMIT)\n",
    "VOCAB_SIZE = len(word_to_idx)\n",
    "PAD_IDX = word_to_idx['<pad>']\n",
    "NUM_CLASS = 4\n",
    "\n",
    "print(f\"Vocabulary size: {VOCAB_SIZE}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "80624701",
   "metadata": {},
   "outputs": [],
   "source": [
    "def collate_batch(batch):\n",
    "    label_list, text_list = [], []\n",
    "    unk_idx = word_to_idx['<unk>']\n",
    "    for (_label, _text) in batch:\n",
    "        label_list.append(int(_label) - 1)\n",
    "        tokens = simple_tokenizer(_text)\n",
    "        indices = [word_to_idx.get(token, unk_idx) for token in tokens]\n",
    "        processed_text = torch.tensor(indices, dtype=torch.int64)\n",
    "        text_list.append(processed_text)\n",
    "        \n",
    "    padded_texts = pad_sequence(text_list, batch_first=True, padding_value=PAD_IDX)\n",
    "    return torch.tensor(label_list, dtype=torch.int64), padded_texts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "423da5d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = NewsDataset(train_data)\n",
    "test_dataset = NewsDataset(test_data)\n",
    "    \n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_batch)\n",
    "test_dataloader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "results = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57f50766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Testing CTM_NLP ---\n",
      "Using neuron select type: random-pairing\n",
      "Synch representation size action: 64\n",
      "Synch representation size out: 128\n",
      "Initializing CTM for NLP tasks...\n",
      "CTM_NLP initialized with vocab_size=10000, max_seq_len=512\n",
      "Output projection layer will map to 10000 logits.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training ctm:   5%|▌         | 94/1875 [02:17<1:00:34,  2.04s/it, loss=0.0499, acc=0.222]"
     ]
    }
   ],
   "source": [
    "# --- Инициализация и обучение CTM ---\n",
    "print(\"\\n--- Testing CTM_NLP ---\")\n",
    "ctm_model = CTM_NLP(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    max_seq_len=MAX_SEQ_LEN,\n",
    "    iterations=CTM_ITERATIONS,\n",
    "    d_model=CTM_D_MODEL,\n",
    "    d_input=CTM_D_INPUT,\n",
    "    # out_dims=NUM_CLASS,\n",
    "    heads=CTM_HEADS,\n",
    "    n_synch_out=CTM_SYNCH_OUT,\n",
    "    n_synch_action=CTM_SYNCH_ACTION,\n",
    "    synapse_depth=CTM_SYNAPSE_DEPTH,\n",
    "    memory_length=CTM_MEMORY_LENGTH,\n",
    "    deep_nlms=True,\n",
    "    memory_hidden_dims=CTM_MEMORY_HIDDEN,\n",
    "    do_layernorm_nlm=False,\n",
    "    dropout=0.2\n",
    ").to(DEVICE)\n",
    "    \n",
    "optimizer_ctm = torch.optim.AdamW(ctm_model.parameters(), lr=LEARNING_RATE)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=PAD_IDX)\n",
    "    \n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc, train_loss = train_epoch(ctm_model, train_dataloader, optimizer_ctm, criterion, model_type='ctm')\n",
    "        test_acc, test_loss = evaluate(ctm_model, test_dataloader, criterion, model_type='ctm')\n",
    "        \n",
    "        print(f'CTM Epoch: {epoch}, Time: {time.time() - epoch_start_time:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tTest Loss:  {test_loss:.4f} | Test Acc:  {test_acc*100:.2f}%')\n",
    "    \n",
    "results['CTM_NLP'] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa6352e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Инициализация и обучение LSTM ---\n",
    "print(\"\\n--- Testing LSTM Baseline ---\")\n",
    "lstm_model = LSTMClassifier(\n",
    "    vocab_size=VOCAB_SIZE,\n",
    "    embed_dim=CTM_D_INPUT,\n",
    "    hidden_dim=LSTM_HIDDEN_DIM,\n",
    "    num_class=NUM_CLASS,\n",
    "    num_layers=LSTM_NUM_LAYERS,\n",
    "    pad_idx=PAD_IDX\n",
    ").to(DEVICE)\n",
    "\n",
    "optimizer_lstm = torch.optim.AdamW(lstm_model.parameters(), lr=LEARNING_RATE)\n",
    "    \n",
    "for epoch in range(1, EPOCHS + 1):\n",
    "        epoch_start_time = time.time()\n",
    "        train_acc, train_loss = train_epoch(lstm_model, train_dataloader, optimizer_lstm, criterion, model_type='lstm')\n",
    "        test_acc, test_loss = evaluate(lstm_model, test_dataloader, criterion, model_type='lstm')\n",
    "        \n",
    "        print(f'LSTM Epoch: {epoch}, Time: {time.time() - epoch_start_time:.2f}s')\n",
    "        print(f'\\tTrain Loss: {train_loss:.4f} | Train Acc: {train_acc*100:.2f}%')\n",
    "        print(f'\\tTest Loss:  {test_loss:.4f} | Test Acc:  {test_acc*100:.2f}%')\n",
    "\n",
    "results['LSTM'] = test_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0d7678",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Итоговое сравнение ---\n",
    "print(\"\\n\" + \"=\"*40)\n",
    "print(\"           FINAL RESULTS\")\n",
    "print(\"=\"*40)\n",
    "print(f\"  CTM_NLP Test Accuracy:  {results.get('CTM_NLP', 0)*100:.2f}%\")\n",
    "print(f\"  LSTM Test Accuracy:     {results.get('LSTM', 0)*100:.2f}%\")\n",
    "print(\"=\"*40)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
